{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduard/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/eduard/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/eduard/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/eduard/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/eduard/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/eduard/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "from tensorflow import lite as tfl\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "use_gdrive = False\n",
    "retrain_models = False\n",
    "\n",
    "if use_gdrive:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Directory in Google Drive were we will save data.\n",
    "    BASE_FOLDER = '/content/drive/My Drive/Colab Notebooks/tfm/'\n",
    "    # Just set to True if you want to retrain the models.\n",
    "    retrain_models = False\n",
    "\n",
    "    drive.mount('/content/drive/')\n",
    "else:\n",
    "    BASE_FOLDER = './'\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('GPU device {} not found'.format(device_name))\n",
    "else:\n",
    "  print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train LYTNet v2\n",
    "path = os.path.abspath(os.getcwd()) + \"/ImVisible/Model/training.py\"\n",
    "#exec(open(path).read())\n",
    "subprocess.run(path, stdout=subprocess.PIPE).stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layers.\n",
    "def conv_layer(x):\n",
    "    x = layers.Conv2D(16, (3, 3), use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    #x = layers.Activation('relu')(x)\n",
    "    x = layers.Activation(tfl.hard_swish)(x)\n",
    "    return x\n",
    "\n",
    "def bottleneck_layer(x):\n",
    "    \n",
    "\n",
    "# Defining the model.\n",
    "def create_model(height, width, depth):\n",
    "    inputShape = (height, width, depth)\n",
    "    x = layers.Input(shape=input_shape)\n",
    "    x = conv_layer(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = bottleneck_layer(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Ejemplo de como cargar de archivos con referencia al directorio base\n",
    "###################################################################################################\n",
    "\n",
    "# DESCARGAR Y DESCOMPRIMIR EL DATASET CALTECH_PREPROCESADO DEL GDRIVE\n",
    "# Cargar las ndarrays\n",
    "with open(BASE_FOLDER+\"pickle_all_images_df.pickle\", \"rb\") as input_file:\n",
    "    x_data = pickle.load(input_file)\n",
    "with open(BASE_FOLDER+\"pickle_all_classes.pickle\", \"rb\") as input_file:\n",
    "    y_data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_data.shape)\n",
    "print(len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image\n",
    "imgplot = plt.imshow(x_data[129])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Shuffle data.\n",
    "aggregated_data = list(zip(x_data, y_data))\n",
    "random.shuffle(aggregated_data)\n",
    "x_data, y_data = zip(*aggregated_data)\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando las labels de texto a valores num√©ricos\n",
    "y = np.unique(y_data)\n",
    "mapping = { key : value for key,value in zip(y,range(len(y)))}\n",
    "processed_y = np.array([mapping[i] for i in y_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = len(y)\n",
    "\n",
    "# Split dataset, 90% train and 10% test.\n",
    "split_1 = int(0.8 * len(y_data))\n",
    "split_2 = int(0.9 * len(y_data))\n",
    "\n",
    "y_train = processed_y[:split_1]\n",
    "x_train = x_data[:split_1]\n",
    "y_validation = processed_y[split_1:split_2]\n",
    "x_validation = x_data[split_1:split_2]\n",
    "y_test = processed_y[split_2:]\n",
    "x_test = x_data[split_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def add_top_model(base_model, print_summary=False):\n",
    "  model = models.Sequential()\n",
    "  model.add(model_base)\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(256, activation='relu'))\n",
    "  model.add(layers.Dense(number_of_classes, activation='softmax'))\n",
    "  if print_summary:\n",
    "    model.summary()\n",
    "  return model\n",
    "\n",
    "# Compile the model.\n",
    "def compile_model(model):\n",
    "  model.compile(optimizer='rmsprop',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30, # grados de rotacion aleatoria\n",
    "    width_shift_range=0.2, # fraccion del total (1) para mover la imagen\n",
    "    height_shift_range=0.2, # fraccion del total (1) para mover la imagen\n",
    "    shear_range=0.15, # deslizamiento\n",
    "    zoom_range=0.15, # rango de zoom\n",
    "    horizontal_flip=True, # girar las imagenes horizontalmente (eje vertical)\n",
    "    fill_mode='nearest', # como rellenar posibles nuevos pixeles\n",
    "    channel_shift_range=0.1 # cambios aleatorios en los canales de la imagen\n",
    ")\n",
    "\n",
    "# Try another one.\n",
    "datagen2 = ImageDataGenerator(\n",
    "    rotation_range=20, # grados de rotacion aleatoria\n",
    "    width_shift_range=0.2, # fraccion del total (1) para mover la imagen\n",
    "    height_shift_range=0.2, # fraccion del total (1) para mover la imagen\n",
    "    shear_range=0.15, # deslizamiento\n",
    "    zoom_range=0.15, # rango de zoom\n",
    "    horizontal_flip=True, # girar las imagenes horizontalmente (eje vertical)\n",
    "    fill_mode='nearest', # como rellenar posibles nuevos pixeles\n",
    "    channel_shift_range=0.2 # cambios aleatorios en los canales de la imagen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Train the model.\n",
    "def fit_model(model, datagen, x_train, y_train, x_validation, y_validation, epochs=20, augmentation=10, callbacks=None, print_plot=True):\n",
    "  batch_size = 32\n",
    "  num_samples = x_train.shape[0] * augmentation\n",
    "  history_pretrained = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                                           epochs=epochs,\n",
    "                                           validation_data=(x_validation, y_validation),\n",
    "                                           use_multiprocessing=True,\n",
    "                                           workers=4,\n",
    "                                           callbacks=callbacks,\n",
    "                                           steps_per_epoch=num_samples//batch_size)\n",
    "\n",
    "  if print_plot:\n",
    "    # Print accuracy plot\n",
    "    epochs = np.arange(1,len(history_pretrained.history['accuracy'])+1)\n",
    "    plt.title('Pre trained model')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(epochs,history_pretrained.history['accuracy'],'r',label='training accuracy')\n",
    "    plt.plot(epochs,history_pretrained.history['val_accuracy'],'b',label='validation accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras.applications import resnet_v2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications import densenet\n",
    "from tensorflow.keras.applications import nasnet\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "# Testing all Keras Applications, to see which ones give better results with our set, and then try to get better results from those.\n",
    "# I excluded NASNetLarge as it just accepts an input of (331, 331, 3) there is an issue about this: https://github.com/keras-team/keras-applications/issues/78\n",
    "# Also same problem with NASNetMobile whitch just accepts an input of (224, 224, 3).\n",
    "pre_trained_models = [Xception, VGG16, VGG19,\n",
    "                      resnet.ResNet50, resnet.ResNet101, resnet.ResNet152,\n",
    "                      resnet_v2.ResNet50V2, resnet_v2.ResNet101V2, resnet_v2.ResNet152V2,\n",
    "                      InceptionV3, InceptionResNetV2, MobileNet,\n",
    "                      densenet.DenseNet121, densenet.DenseNet169, densenet.DenseNet201,\n",
    "                      #nasnet.NASNetLarge, nasnet.NASNetMobile,\n",
    "                      MobileNetV2]\n",
    "\n",
    "for pre_trained_model in pre_trained_models:\n",
    "  model_base = pre_trained_model(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "  print('Pre trained model: ', model_base.name)\n",
    "  if retrain_models or not os.path.isfile(BASE_FOLDER+model_base.name+'.h5'):\n",
    "    model_base.trainable = False\n",
    "    model = add_top_model(model_base, False)\n",
    "    compile_model(model)\n",
    "    fit_model(model, datagen, x_train, y_train, x_validation, y_validation, 5, 5);\n",
    "    model.save(BASE_FOLDER+model_base.name+'.h5')\n",
    "  else:\n",
    "    model = load_model(BASE_FOLDER+model_base.name+'.h5')\n",
    "  # Let's evaluate it.\n",
    "  loss, accuracy = model.evaluate(x_test, y_test)\n",
    "  print('loss {} accuracy {}'.format(loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like the best results came from VGG16 and VGG19, we can not really be sure with that low number of epochs, but I'll follow my gut.\n",
    "# With more time and more processing power, I would test well every one of them.\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "model_name = 'trained_vgg16'\n",
    "if retrain_models or not os.path.isfile(BASE_FOLDER+model_name+'.h5'):\n",
    "  # Define model using VGG16 to transfer learning.\n",
    "  model_base = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "  model_base.trainable = False\n",
    "  # Add a top model to the base_model.\n",
    "  model = add_top_model(model_base, False)\n",
    "\n",
    "  # Compile the model.\n",
    "  compile_model(model)\n",
    "\n",
    "  # Save checkpoints to get the best weights from all epochs, same name for all, to keep the best checkpoint.\n",
    "  filename = BASE_FOLDER+model_name+'checkpoint.hdf5'\n",
    "  checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "  callbacks = [checkpoint]\n",
    "\n",
    "  # Fit the model.\n",
    "  fit_model(model, datagen, x_train, y_train, x_validation, y_validation, 20, 10, callbacks);\n",
    "\n",
    "  # Load best weights.\n",
    "  model.load_weights(filename)\n",
    "\n",
    "  # Compile the model with the best weights.\n",
    "  compile_model(model)\n",
    "\n",
    "  # Save the model.\n",
    "  model.save(BASE_FOLDER+model_name+'.h5')\n",
    "else:\n",
    "  # Load the model.\n",
    "  model = load_model(BASE_FOLDER+model_name+'.h5')\n",
    "\n",
    "# Let's evaluate it.\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('loss {} accuracy {}'.format(loss, accuracy))\n",
    "\n",
    "# Gives our best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "# Show image\n",
    "img_index = 2\n",
    "img = x_data[img_index]\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# Make a prediction.\n",
    "predictions = model.predict(np.array([img]))\n",
    "prediction = np.argmax(predictions)\n",
    "\n",
    "print('Predicted: ', y[prediction])\n",
    "print('Real: ', y[processed_y[img_index]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now learning from scratch.\n",
    "# I been trying a fiew models, and the following 2 where the ones to give better results.\n",
    "\n",
    "model_name = 'custom_model_1'\n",
    "if retrain_models or not os.path.isfile(BASE_FOLDER+model_name+'.h5'):\n",
    "  # Define model.\n",
    "  model = models.Sequential()\n",
    "  # The input has 128 for 128 pixels and 3 channels, we stay that.\n",
    "  # The number of channels for each convolutional layer has been chosen using trial and error, starting with a number higher than 32 or lower than 16 was giving bad results\n",
    "  model.add(layers.Conv2D(32,(3,3),input_shape=(128,128,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "  # We use MaxPooling to reduce dimensionality, we group each 4 pixels and use tha max.\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  model.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  # Increase the numeber of channels slowly while we reduce dimensionality.\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  # End of the base model.\n",
    "  # Start of the top model.\n",
    "  # We flaten to one dimension.\n",
    "  model.add(layers.Flatten())\n",
    "  # One Dense of 256 was giving the best result.\n",
    "  model.add(layers.Dense(256,activation='relu'))\n",
    "  # Softmax activation with the number of clases we want to classify.\n",
    "  model.add(layers.Dense(number_of_classes,activation='softmax'))\n",
    "  #model.summary()\n",
    "\n",
    "  # Compile the model.\n",
    "  compile_model(model)\n",
    "\n",
    "  # Save checkpoints to get the best weights from all epochs, same name for all, to keep the best checkpoint.\n",
    "  filename = BASE_FOLDER+model_name+'checkpoint.hdf5'\n",
    "  checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "  callbacks = [checkpoint]\n",
    "\n",
    "  # Fit the model.\n",
    "  fit_model(model, datagen, x_train, y_train, x_validation, y_validation, 20, 10, callbacks);\n",
    "\n",
    "  # Load best weights.\n",
    "  model.load_weights(filename)\n",
    "\n",
    "  # Compile the model with the best weights.\n",
    "  compile_model(model)\n",
    "\n",
    "  # Save the model.\n",
    "  model.save(BASE_FOLDER+model_name+'.h5')\n",
    "else:\n",
    "  # Load the model.\n",
    "  model = load_model(BASE_FOLDER+model_name+'.h5')\n",
    "\n",
    "# Let's evaluate it.\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('loss {} accuracy {}'.format(loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another model learning from scratch.\n",
    "\n",
    "model_name = 'custom_model_2'\n",
    "if retrain_models or not os.path.isfile(BASE_FOLDER+model_name+'.h5'):\n",
    "  # Define model.\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Conv2D(16,(3,3),input_shape=(128,128,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(16,(3,3),activation='relu'))\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  model.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "  model.add(layers.MaxPooling2D((2,2)))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(256,activation='relu'))\n",
    "  model.add(layers.Dense(number_of_classes,activation='softmax'))\n",
    "  #model.summary()\n",
    "\n",
    "  # Compile the model.\n",
    "  compile_model(model)\n",
    "\n",
    "  # Save checkpoints to get the best weights from all epochs, same name for all, to keep the best checkpoint.\n",
    "  filename = BASE_FOLDER+model_name+'checkpoint.hdf5'\n",
    "  checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "  callbacks = [checkpoint]\n",
    "\n",
    "  # Fit the model.\n",
    "  fit_model(model, datagen, x_train, y_train, x_validation, y_validation, 20, 10, callbacks);\n",
    "\n",
    "  # Load best weights.\n",
    "  model.load_weights(filename)\n",
    "\n",
    "  # Compile the model with the best weights.\n",
    "  compile_model(model)\n",
    "\n",
    "  # Save the model.\n",
    "  model.save(BASE_FOLDER+model_name+'.h5')\n",
    "else:\n",
    "  # Load the model.\n",
    "  model = load_model(BASE_FOLDER+model_name+'.h5')\n",
    "\n",
    "# Let's evaluate it.\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('loss {} accuracy {}'.format(loss, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
